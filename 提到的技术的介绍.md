# 官方文档里面的技术

## 欠下的概览
1. 硬件架构上：
* 设计做什么运算 某种运算多一些 => 没找到；但是找到了关于Thread和矩阵分块的描述，理论上这部分的合理规划可以提升性能，例如规定每次分块的大小和参与计算的矩阵的分块关系以及和总的cuda core的数量的关系 => 这部分软件上大概很难实现，需要GPU编程
2. 软件上：
* 向量运算块，标量运算慢：算法优化 =>数组运算转化为矩阵运算；
* 关于维度的操作，理论上最好宽度时byte数的整数倍，也就是16的整数倍; 然而文档是基于Tensor Core的, 不知道对于Cuda Core有没有用
* 和OpenCV相关的算法的简化：VPI;
* 和数据预处理有关的算法的简化：DALI;
* 基于混合精度的训练和模型的运行; -- 似乎硬件的版本不支持 ==> 九牛二虎之力找到了
* 模型的量化: 完成demo就算成功! (失败了)
* 理论上讲通过增大矩阵的维度可以更好的利用GPU，但是得失感觉不太匹配啊;
* 替换FC层：一般情况是怎样计算的？ -- TensorRT会自动计算, 而且现在已经是很简单的FC层了, 不需要
3. TensorRT使用技巧
* trtexec使用的一些参数 √ 
* 一堆测试的方法 (差一种) 
* 层的融合, 怎么做, 有什么用 => 自动进行的
* 中间也可以进行量化还是什么的? => QAT生成新的onnx模型后在TensorRT用int8/fp16计算, 会有特殊的量化节点的方法

* 生成动态shape的模型
## Quantization-aware-training(QAT)

本质上就是对网络的规模进行压缩, 将训练好的网络中的模型参数进行精度的降低, 可以减少之后推理时的运算成本和模型的大小

QAT在训练时引入了 ***伪量化*** 来模拟量化误差, 减少之后的模型精度损失

### 原理

将浮点数(FP32)映射到目标的精度的(INT8)的区间, 概括为转化的式子就是$r = S(q - Z)$, 其中r, q是变化前和变化后的值, S和Z为需要确定的值

如果按照最简单的办法进行投影的话, 最值分别进行仿射变化, 得到量化公式$q = \frac{r}{S} + Z$, $S = \frac{r_{max} - r_{min}}{q_{max} - q_{min}}$ 在实际运算的时候是上面的量化和反量化互相转化, 因为不同的算子可能需要输入的精确度不同

相比于普通的量化方法, QAT在训练的时候插入 ***伪量化*** 节点, 训练网络本身和最后生成量化系数的参数, 减少精度的损失, 伪量化节点的作用是在训练的过程中模拟实际量化产生的误差, 作为训练的损失的一部分, 伪量化遵循的公式为

$
q(r; r_{max}, r_{min}, n) = \lceil\frac{clamp(r;r_{max}, r_{min}) - r_{min}}{s( r_{max}, r_{min}, n)}\rfloor \ s( r_{max}, r_{min}, n) + r_{min}
$

$
clamp( r_{max}, r_{min}, x) = \begin{cases} r_{min} & x \leq r_{min} \\ x & r_{min}  \leq x \leq r_{max} \\ r_{max} & x \geq r_{max} \end{cases}
$

$
s( r_{max}, r_{min}, n) = \frac{r_{max} - r_{min}}{n - 1}
$

大致流程如下图所示

但是, 实际实现的时候还需要考虑在forward, activation中的具体操作和在backward时可能涉及的操作, 并且, 由于网络通常带有batchnorm, 且量化和batchnorm本质上都是线性操作, 理论上存在网络的结构融合的可能, 所以可以通过重写前推的表达

代码实现思路(找到的太长了, 先不复现了)
[![大致思路](https://s1.ax1x.com/2022/12/05/zsxUaR.jpg)](https://imgse.com/i/zsxUaR)
目前这个完整的代码的版本是包括所有的层全部进行量化

由于所有层的计算都比原来多了好几步, 所有必须重写卷积层之类的需要量化的层, 这些层的基本的前推的流程为
* 对`weight`按照直接量化的方法量化, 对`input`按照滑动量化的方法量化
* 前推
如果层采用了融合的方式, 则需要前推后重新算结果, 并且最后bp的结果也要重算

如果已经实现了经过量化训练后的各个新的层, 那么在实际训练的时候只需要两步
* 将原先经过预训练的网络copy给要量化的网络, 并且将原先网络的层进行替换
* 训练新的插入伪量化节点后的网络, 得到量化参数和适应训练的weight
* 将网络进行量化 ??
* 放到对应的推理框架上进行推理

### 问题

该量化方案复现失败了, 失败的地方在于量化后的网络的大小没有减小, 而且推理速度也没有加快, 目前的问题有

* 在量化后进行保存和实际推理时的网络究竟是哪一个, 是否保存有伪量化节点
* 在训练好之后怎样对模型进行量化以及保存, 量化后所有参数的类型都变成INT8, 还是到推理框架上才变成INT8? 会不会有输入类型不匹配报错的可能(INT8和float32不能一起计算)

## 混合精度训练(amp)

原理见[NVIDIA中官方手册介绍记录](/Users/liuchang/Desktop/MachineLearning/JetsonNano/官方手册/recordme.md), 实现时使用包`apex.amp`

使用amp主要分为三步, 分别是
* 使用不同的参数初始化amp, `amp.initialize`, 此时的输入是模型和优化器, 因此需要 ***预先定义*** 好这两个, 常常设置的参数包括
  * 优化等级`opt_level`, 包括`O0`到`O4`
    * `O0`: `FP32`, 所有的都是fp32的格式
    * `O1`: 混合精度训练, 将所有PyTorch支持FP16运算的都用FP16, 但是保存的都是FP32
    * `O2`: 基本是混合精度训练, 所有的都用FP16
    * `O3`: 完全FP16精确度可能会相应的降低
  * 是否保持输出是某种格式
  * 是否保持batchnorm是fp32类型
  * 损失的量级`loss_scale`, 输入为"dynamic"或者数字
* 用`amp`中定义的`apex.parallel.DistributedDataParallel`代替`nn.DistributedDataParallel`
* 在训练的损失进行`backward`的时候, 使用amp定义的`scaled_loss`, 而不是直接计算的loss, 计算损失的形式是`amp.scale_loss(loss, optimizer)`

### 混合精度训练和分布式训练联合使用

#### PyTorch的nn.DistributedDataParallel

使用与多线程, 多GPU, 每个GPU由一个线程控制, 各个GPU之间通信的内容是梯度. 推理的过程为

* 每个进程加载数据分别加载`minibatch`, 传给自己的GPU
* GPU单独forward
* 梯度在GPU之间计算, 并行backward
  * 每个GPU有 ***平均*** 梯度, 保持`synchronized`

为了实现不同的进程以及对不同的进程进行同步和通信, 需要PyTorch可以(对应的函数是`nn.utils.data.DistributedSampler`)
* 找到`process 0`
* 同步多少进程`world size`
* 每个进程所在的阶序`rank`以及对应的GPU, 对应数据的哪一部分

对应在程序中需要修改的是([参考程序](./distributed.py))
* 计算`world_size = node * gpu`, 其中`node`代表几台机器, `gpu`代表单机的卡数
* 设置通信的地址`os.environ['MASTER_ADDR']`和端口`os.environ['MASTER_PORT']`
  * 对于单机来说直接用默认的地址`127.0.0.1`和端口`29500`就可以
* 生成进程, 每个进程对应一块GPU
  * 使用的函数为`mp.spawn`, 要求此时程序的启动方式必须是` if __name__=="__main__"`

在训练中需要修改的是
* 在开始前初始化所有的进程, 并初始化进程组
* 将模型打包成DDP模式, 指定模型发送到哪个GPU上
* 将数据加载器中导入`DistributedSampler`, 保证每个进程拿到的是不同的训练数据, 并且不shuffle

若要保证在使用分布式计算和之前的参数保持一致, 需要注意调整
* 学习率: 使用多卡的学习率相当于
* batch size

在 ***保存*** 训练得到的模型参数时需要注意, 由于在不同的进程时显然只需要保存一次模型, 因此这里需要指定特定的进程来完成模型的保存
* `torch.distributed.get_rank()`

在 ***加载*** 模型时需要注意, 最好一开始加载在CPU上, 之后在分布式的分配到不同的进程中

#### 使用amp混合
见上

## trtexec

主要作用: 生成序列化的推理引擎, 也即生成`trt`文件

`/home/developers/liuchang/dl_env/TensorRT-7.0.0.11/samples/trtexec`

里面有文档和profile和trace(然而看不懂输出是什么)

根据文档里面提到的, 理论上使用`trtexec`可以实现多种功能, 除了直接转化以外, 还包括
* 测试每一层的性能, 但是需要为要测试的层注册`plugin`, 如果这个层是TensorRT中包含的动作, 需要load`libnvinfer_plugin.so`
* 创建具有动态输入shape的推理引擎, 在这一步可以给它一系列不同的输入的大小, 或者直接给出输入的范围
* 输出可视化的推理时间的示意图之类的

当trtexec运行时会输出在推理过程中的测试时间, 不同的输出的含义是
* Host Latency: 输入 + 计算 + 输出 总耗时
* Troughput
* GPU compute: GPU计算的耗时
[![输出结果示例](https://s1.ax1x.com/2022/12/05/zy1qv8.png)](https://imgse.com/i/zy1qv8)
除此之外`trtexec`的其它常用参数还有
* 关于Shape的: `minShape`, `maxShape`, `optShape`, 如果onnx模型设置的batch_size是动态, 则可以设置这里来规定不同的batch的大小, 理论上某些batch的大小是可以达到更优的
* workspace: 在TensorRT优化时使用的控件, 单位是`MB`, 一般默认的都不够, 最好设置的大一点
* fp16: 允许精度为fp16的运算
* streams: 多个流同时运行, 以延时增大为代价增大吞吐量(?新的引擎的输入是什么样的, 是对应的多张图片吗?)
* verbose: 输出详细的日志
* exportTimes, exportProfile: 将时间信息和测试信息保存在json文件里面

在整个生成引擎的过程中还会输出在中间尝试的不同的trick以及最终形成的网络的结构, 所以可以对使用不同的trick简化后的模型进行可视化, 顺带加上生成的profile文件, 整个可视化过程在`/home/developers/liuchang/toRT/vis`

## 测试方法

* 测试CPU的: 跳过
* 测试profile的: 和trtexec有重合, 跳过
* 使用`Nsight System`

### Nsight System

* 使用`NVTX`跟踪 ***Nsight Compute*** 和 ***Nsight Systems*** 收集数据, 可以标记事件和范围
* 在jetson上使用的时候可以先saveEngine, save的时候`nvtx`跟踪模式使用`verbose`模式, 也即`./trtexec --onnx= --saveEngine= --nvtxMode=verbose --dumpProfile`
* 生成保留有`nvtx`记录的引擎后重新使用Nsight System跑一次生成波形记录文件`.nsys-rep`, 即`nsys profile -o <out_profile> trtexec --loadEngine== ==warmUp=0 --duration=0 --iterations=50`
* 然而生成的波形有点看不懂, 尴尬😢😢😢 可以通过查找这个node的描述信息, 根据输入的编号和层的名字以及网络的可视化的图形来确定是在哪一步的那一层做了这个优化

### Tracking Memory

理论上: 创建GPU分配器, 使用内存分配函数

## 层的融合以及对FC的处理

### 全连接层

需要知道: 在TensorRT中是如何处理这两类层的, 其自动处理达到了什么效果以及什么先决条件, 是否可以在改变某些网络结构后进一步压缩模型

如果不考虑TensorRT可能做的优化, 理论上可以将全连接层的运算转化为纯粹的卷积运算, 不过二者在计算时实际上都是卷积运算, 不知道实际效果如何

修改方法1: 使用`1 * 1`卷积代替: 参数量缩小幅度和最后特征图的大小相关, 所以本网络里面没用

仔细一看这个网络就是用的 ***全局平均池化***(优化方法的一种), 所以最后才可以把维度压缩到`1 * 1`

感觉没啥前景, 还是使用并行化更可靠一点

### 层的融合

确实是自动做的, 会做的操作都在手册里面写的, 但是看起来感觉已经融合了不少了, 没必要再加了, 最多的就是卷积, BN, Relu融合

倒是量化部分做的比较特殊, 会对输出的量化和反量化节点做融合, 但是如果是自己写的, 可以被识别为量化节点吗? 这里还需要研究

## 并行化

需要决定目前的方案在读取数据和推理结束后的处理时间是否占用了很多时间(目前根据测试结果还好)

分为两种并行化的方式, 分别是
* 增大batch_size, 增大GPU的运算效率, 尤其是8的倍数的batch
* 使用多个stream, 不同的核可以计算同样计算的不同阶段
* 多线程
理论上这两种方法都是可以增大吞吐量

## DALI: Data Load Library 

见[GitHub链接](https://github.com/ruachang/DALI)

